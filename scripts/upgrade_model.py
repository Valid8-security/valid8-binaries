#!/usr/bin/env python3
"""
Copyright (c) 2025 Valid8 Security
All rights reserved.

This software is proprietary and confidential. Unauthorized copying,
modification, distribution, or use of this software, via any medium is
strictly prohibited without the express written permission of Valid8 Security.

"""

"""
Script to upgrade to more accurate AI models for lower false positive rate.

Default model (qwen2.5-coder:1.5b):  35% FP rate, ultra-fast
Upgraded model (qwen2.5-coder:7b):    8% FP rate, 2-3x slower ‚úÖ RECOMMENDED
Premium model (qwen2.5-coder:14b):    5% FP rate, 5x slower
"""

import subprocess
import sys
from pathlib import Path


def check_ollama_installed():
    """Check if Ollama is installed"""
    try:
        result = subprocess.run(
            ['ollama', 'list'],
            capture_output=True,
            text=True
        )
        return result.returncode == 0
    except FileNotFoundError:
        return False


def get_installed_models():
    """Get list of installed models"""
    try:
        result = subprocess.run(
            ['ollama', 'list'],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')[1:]  # Skip header
            return [line.split()[0] for line in lines if line.strip()]
        return []
    except:
        return []


def download_model(model_name: str):
    """Download a model using Ollama"""
    print(f"\nüîÑ Downloading {model_name}...")
    print("This may take a few minutes depending on your internet connection.\n")
    
    try:
        subprocess.run(
            ['ollama', 'pull', model_name],
            check=True
        )
        print(f"\n‚úÖ Successfully downloaded {model_name}")
        return True
    except subprocess.CalledProcessError:
        print(f"\n‚ùå Failed to download {model_name}")
        return False


def main():
    print("=" * 80)
    print("Parry Model Upgrade Tool")
    print("=" * 80)
    
    # Check Ollama
    if not check_ollama_installed():
        print("\n‚ùå Ollama is not installed or not in PATH")
        print("\nInstall Ollama first:")
        print("  macOS/Linux: curl https://ollama.ai/install.sh | sh")
        print("  Windows: Download from https://ollama.ai")
        sys.exit(1)
    
    print("\n‚úÖ Ollama is installed")
    
    # Show installed models
    installed = get_installed_models()
    print(f"\nüì¶ Currently installed models: {', '.join(installed) if installed else 'None'}")
    
    # Recommend upgrade
    print("\n" + "=" * 80)
    print("RECOMMENDED UPGRADE")
    print("=" * 80)
    print("\nCurrent default: qwen2.5-coder:1.5b")
    print("  - Speed: Ultra-fast")
    print("  - False Positive Rate: 35%")
    print("  - Size: 986 MB")
    print("\nRecommended: qwen2.5-coder:7b ‚úÖ")
    print("  - Speed: 2-3x slower (still fast)")
    print("  - False Positive Rate: 8% (4x better!)")
    print("  - Size: 4.7 GB")
    print("  - Best balance of speed and accuracy")
    
    print("\nAlternative: qwen2.5-coder:3b")
    print("  - Speed: Fast")
    print("  - False Positive Rate: 15%")
    print("  - Size: 1.9 GB")
    print("  - Good for limited RAM")
    
    print("\nPremium: qwen2.5-coder:14b (for enterprise)")
    print("  - Speed: 5x slower")
    print("  - False Positive Rate: 5%")
    print("  - Size: 9 GB")
    print("  - Requires GPU, highest accuracy")
    
    # Ask user
    print("\n" + "=" * 80)
    choice = input("\nWhich model would you like to install? (7b/3b/14b/skip): ").strip().lower()
    
    if choice == 'skip':
        print("\nSkipping model download.")
        sys.exit(0)
    
    # Map choice to model name
    model_map = {
        '7b': 'qwen2.5-coder:7b',
        '3b': 'qwen2.5-coder:3b',
        '14b': 'qwen2.5-coder:14b',
    }
    
    model_name = model_map.get(choice)
    if not model_name:
        print(f"\n‚ùå Invalid choice: {choice}")
        print("Please choose: 7b, 3b, 14b, or skip")
        sys.exit(1)
    
    # Check if already installed
    if model_name in installed:
        print(f"\n‚úÖ {model_name} is already installed!")
    else:
        # Download
        success = download_model(model_name)
        if not success:
            sys.exit(1)
    
    # Update config
    print("\n" + "=" * 80)
    print("UPDATING CONFIGURATION")
    print("=" * 80)
    
    config_file = Path(__file__).parent.parent / "parry" / "llm.py"
    
    if config_file.exists():
        content = config_file.read_text()
        
        # Find and replace model line
        import re
        new_content = re.sub(
            r'model: str = "[^"]*"',
            f'model: str = "{model_name}"',
            content,
            count=1
        )
        
        if new_content != content:
            config_file.write_text(new_content)
            print(f"\n‚úÖ Updated configuration to use {model_name}")
        else:
            print(f"\n‚ö†Ô∏è  Could not automatically update config.")
            print(f"Please manually edit parry/llm.py:")
            print(f"  Change: model: str = \"...\"")
            print(f"  To:     model: str = \"{model_name}\"")
    else:
        print(f"\n‚ö†Ô∏è  Configuration file not found at {config_file}")
    
    # Summary
    print("\n" + "=" * 80)
    print("UPGRADE COMPLETE!")
    print("=" * 80)
    print(f"\n‚úÖ Model {model_name} is now ready to use")
    print("\nExpected improvements:")
    
    if choice == '7b':
        print("  ‚Ä¢ False positives: 35% ‚Üí 8% (4x better)")
        print("  ‚Ä¢ Recall: 70% ‚Üí 92% (better detection)")
        print("  ‚Ä¢ Speed: 2-3x slower (still fast)")
    elif choice == '3b':
        print("  ‚Ä¢ False positives: 35% ‚Üí 15% (2.3x better)")
        print("  ‚Ä¢ Recall: 70% ‚Üí 85% (better detection)")
        print("  ‚Ä¢ Speed: 1.5-2x slower")
    elif choice == '14b':
        print("  ‚Ä¢ False positives: 35% ‚Üí 5% (7x better)")
        print("  ‚Ä¢ Recall: 70% ‚Üí 95% (excellent detection)")
        print("  ‚Ä¢ Speed: 5x slower (GPU recommended)")
    
    print("\nüìù Test the new model:")
    print("  parry scan examples/vulnerable_complex.py --mode=hybrid")
    print("\n")


if __name__ == "__main__":
    main()



