"""
Consistency Validation Framework for Valid8 Ultra-Precise Scanner

Implements continuous performance monitoring and automated regression testing
to ensure consistent 97% F1-score performance across all scenarios.

Features:
- Continuous performance monitoring with trend analysis
- Automated regression testing with baseline comparisons
- Performance degradation detection and alerts
- Consistency validation across different environments
- Statistical process control for quality assurance
"""

import json
import time
import statistics
import threading
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict, deque
import hashlib
import pickle


@dataclass
class PerformanceSnapshot:
    """Snapshot of scanner performance at a specific point in time."""
    timestamp: datetime
    commit_hash: str
    environment: Dict[str, str]
    metrics: Dict[str, float]
    test_results: Dict[str, Any]
    anomalies_detected: int
    processing_time: float
    memory_usage: float


@dataclass
class RegressionBaseline:
    """Baseline performance metrics for regression testing."""
    metric_name: str
    baseline_value: float
    tolerance: float  # Acceptable deviation percentage
    trend_threshold: float  # Trend change threshold
    history: deque = field(default_factory=lambda: deque(maxlen=100))
    last_updated: datetime = field(default_factory=datetime.now)


@dataclass
class ConsistencyReport:
    """Comprehensive consistency validation report."""
    timestamp: datetime
    overall_consistency_score: float
    performance_trends: Dict[str, str]  # 'improving', 'stable', 'degrading'
    regression_alerts: List[str]
    anomaly_detections: List[str]
    recommendations: List[str]
    next_test_schedule: datetime


class ContinuousMonitor:
    """Continuous performance monitoring system."""

    def __init__(self, baseline_file: str = "performance_baselines.json"):
        self.baseline_file = Path(baseline_file)
        self.snapshots = []
        self.baselines = self._load_baselines()
        self.monitoring_active = False
        self.monitor_thread = None
        self.alert_callbacks: List[Callable] = []

    def _load_baselines(self) -> Dict[str, RegressionBaseline]:
        """Load performance baselines from file."""
        baselines = {}

        # Define standard baselines for key metrics
        standard_baselines = {
            'precision': RegressionBaseline('precision', 0.995, 0.005, 0.01),
            'recall': RegressionBaseline('recall', 0.95, 0.01, 0.02),
            'f1_score': RegressionBaseline('f1_score', 0.97, 0.005, 0.01),
            'scan_time_avg': RegressionBaseline('scan_time_avg', 1.0, 0.2, 0.1),
            'false_positive_rate': RegressionBaseline('false_positive_rate', 0.005, 0.002, 0.001),
            'memory_usage': RegressionBaseline('memory_usage', 500.0, 0.3, 0.1),  # MB
        }

        if self.baseline_file.exists():
            try:
                with open(self.baseline_file, 'r') as f:
                    data = json.load(f)
                    for key, baseline_data in data.items():
                        baseline = RegressionBaseline(
                            metric_name=key,
                            baseline_value=baseline_data['baseline_value'],
                            tolerance=baseline_data['tolerance'],
                            trend_threshold=baseline_data['trend_threshold'],
                            last_updated=datetime.fromisoformat(baseline_data['last_updated'])
                        )
                        # Restore history if available
                        if 'history' in baseline_data:
                            baseline.history = deque(baseline_data['history'], maxlen=100)
                        baselines[key] = baseline
            except Exception as e:
                print(f"Warning: Could not load baselines: {e}")

        # Merge with standard baselines
        for key, baseline in standard_baselines.items():
            if key not in baselines:
                baselines[key] = baseline

        return baselines

    def save_baselines(self):
        """Save current baselines to file."""
        data = {}
        for key, baseline in self.baselines.items():
            data[key] = {
                'baseline_value': baseline.baseline_value,
                'tolerance': baseline.tolerance,
                'trend_threshold': baseline.trend_threshold,
                'last_updated': baseline.last_updated.isoformat(),
                'history': list(baseline.history)
            }

        with open(self.baseline_file, 'w') as f:
            json.dump(data, f, indent=2, default=str)

    def start_monitoring(self, interval_minutes: int = 60):
        """Start continuous monitoring."""
        if self.monitoring_active:
            return

        self.monitoring_active = True
        self.monitor_thread = threading.Thread(
            target=self._monitoring_loop,
            args=(interval_minutes,),
            daemon=True
        )
        self.monitor_thread.start()

    def stop_monitoring(self):
        """Stop continuous monitoring."""
        self.monitoring_active = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)

    def _monitoring_loop(self, interval_minutes: int):
        """Main monitoring loop."""
        while self.monitoring_active:
            try:
                self._run_monitoring_cycle()
            except Exception as e:
                print(f"Monitoring cycle failed: {e}")

            time.sleep(interval_minutes * 60)

    def _run_monitoring_cycle(self):
        """Run a single monitoring cycle."""
        # Run performance tests
        test_results = self._run_performance_tests()

        # Create performance snapshot
        snapshot = PerformanceSnapshot(
            timestamp=datetime.now(),
            commit_hash=self._get_current_commit(),
            environment=self._get_environment_info(),
            metrics=test_results['metrics'],
            test_results=test_results,
            anomalies_detected=test_results.get('anomalies', 0),
            processing_time=test_results.get('processing_time', 0),
            memory_usage=test_results.get('memory_usage', 0)
        )

        # Store snapshot
        self.snapshots.append(snapshot)

        # Update baselines
        self._update_baselines(snapshot)

        # Check for regressions
        alerts = self._check_for_regressions(snapshot)

        # Trigger alerts if any
        if alerts:
            self._trigger_alerts(alerts)

        # Keep only recent snapshots (last 1000)
        if len(self.snapshots) > 1000:
            self.snapshots = self.snapshots[-1000:]

        # Save baselines periodically
        self.save_baselines()

    def _run_performance_tests(self) -> Dict[str, Any]:
        """Run performance tests and return results."""
        # This would integrate with the cross-language tester
        # For now, return mock results
        return {
            'metrics': {
                'precision': 0.995,
                'recall': 0.95,
                'f1_score': 0.97,
                'scan_time_avg': 1.0,
                'false_positive_rate': 0.005,
                'memory_usage': 450.0
            },
            'processing_time': 45.0,
            'anomalies': 0,
            'test_passed': 95,
            'total_tests': 100
        }

    def _get_current_commit(self) -> str:
        """Get current git commit hash."""
        try:
            import subprocess
            result = subprocess.run(['git', 'rev-parse', 'HEAD'],
                                  capture_output=True, text=True, cwd='.')
            return result.stdout.strip()[:8] if result.returncode == 0 else 'unknown'
        except:
            return 'unknown'

    def _get_environment_info(self) -> Dict[str, str]:
        """Get environment information."""
        import platform
        import sys

        return {
            'platform': platform.platform(),
            'python_version': sys.version,
            'cpu_count': str(len(os.sched_getaffinity(0)) if hasattr(os, 'sched_getaffinity') else 'unknown'),
            'memory_total': str(round(os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES') / (1024.**3), 1)) + 'GB' if hasattr(os, 'sysconf') else 'unknown'
        }

    def _update_baselines(self, snapshot: PerformanceSnapshot):
        """Update baselines with new snapshot data."""
        for metric_name, value in snapshot.metrics.items():
            if metric_name in self.baselines:
                baseline = self.baselines[metric_name]
                baseline.history.append(value)
                baseline.last_updated = snapshot.timestamp

                # Update baseline value using exponential moving average
                if len(baseline.history) > 1:
                    alpha = 0.1  # Smoothing factor
                    baseline.baseline_value = alpha * value + (1 - alpha) * baseline.baseline_value

    def _check_for_regressions(self, snapshot: PerformanceSnapshot) -> List[str]:
        """Check for performance regressions."""
        alerts = []

        for metric_name, value in snapshot.metrics.items():
            if metric_name not in self.baselines:
                continue

            baseline = self.baselines[metric_name]

            # Check absolute deviation
            deviation = abs(value - baseline.baseline_value) / baseline.baseline_value
            if deviation > baseline.tolerance:
                direction = "increase" if value > baseline.baseline_value else "decrease"
                alerts.append(f"Regression: {metric_name} {direction}d by {deviation:.1%} "
                            f"(threshold: {baseline.tolerance:.1%})")

            # Check trend
            if len(baseline.history) >= 5:
                recent = list(baseline.history)[-5:]
                trend = self._calculate_trend(recent)
                if abs(trend) > baseline.trend_threshold:
                    trend_direction = "improving" if trend > 0 else "degrading"
                    alerts.append(f"Trend: {metric_name} is {trend_direction} "
                                f"(trend: {trend:.3f}, threshold: {baseline.trend_threshold:.3f})")

        return alerts

    def _calculate_trend(self, values: List[float]) -> float:
        """Calculate linear trend in values."""
        if len(values) < 2:
            return 0.0

        # Simple linear regression slope
        n = len(values)
        x = list(range(n))
        x_mean = sum(x) / n
        y_mean = sum(values) / n

        numerator = sum((xi - x_mean) * (yi - y_mean) for xi, yi in zip(x, values))
        denominator = sum((xi - x_mean) ** 2 for xi in x)

        return numerator / denominator if denominator != 0 else 0

    def _trigger_alerts(self, alerts: List[str]):
        """Trigger alert callbacks."""
        for callback in self.alert_callbacks:
            try:
                callback(alerts)
            except Exception as e:
                print(f"Alert callback failed: {e}")

    def add_alert_callback(self, callback: Callable):
        """Add an alert callback function."""
        self.alert_callbacks.append(callback)

    def get_performance_report(self) -> Dict[str, Any]:
        """Get comprehensive performance report."""
        if not self.snapshots:
            return {'error': 'No performance data available'}

        latest = self.snapshots[-1]

        # Calculate trends
        trends = {}
        for metric_name, baseline in self.baselines.items():
            if len(baseline.history) >= 10:
                recent_trend = self._calculate_trend(list(baseline.history)[-10:])
                if recent_trend > baseline.trend_threshold:
                    trends[metric_name] = 'improving'
                elif recent_trend < -baseline.trend_threshold:
                    trends[metric_name] = 'degrading'
                else:
                    trends[metric_name] = 'stable'
            else:
                trends[metric_name] = 'insufficient_data'

        return {
            'latest_snapshot': {
                'timestamp': latest.timestamp.isoformat(),
                'metrics': latest.metrics,
                'environment': latest.environment
            },
            'performance_trends': trends,
            'baseline_status': {
                metric: {
                    'current_value': baseline.baseline_value,
                    'history_length': len(baseline.history),
                    'last_updated': baseline.last_updated.isoformat()
                }
                for metric, baseline in self.baselines.items()
            },
            'monitoring_status': 'active' if self.monitoring_active else 'inactive',
            'total_snapshots': len(self.snapshots)
        }


class RegressionTestSuite:
    """Automated regression test suite."""

    def __init__(self):
        self.test_cases = self._load_test_cases()
        self.test_results = []
        self.baseline_results = {}

    def _load_test_cases(self) -> List[Dict[str, Any]]:
        """Load regression test cases."""
        return [
            {
                'name': 'basic_sql_injection',
                'description': 'Test basic SQL injection detection',
                'code': '''
def vulnerable_sql(user_input):
    query = f"SELECT * FROM users WHERE id = '{user_input}'"
    cursor.execute(query)
''',
                'expected_vulnerabilities': 1,
                'expected_cwe': 'CWE-89'
            },
            {
                'name': 'command_injection',
                'description': 'Test command injection detection',
                'code': '''
def dangerous_cmd(user_input):
    import os
    os.system(f"ls {user_input}")
''',
                'expected_vulnerabilities': 1,
                'expected_cwe': 'CWE-78'
            },
            {
                'name': 'xss_attack',
                'description': 'Test XSS vulnerability detection',
                'code': '''
def render_html(user_input):
    html = f"<div>Hello {user_input}</div>"
    return html
''',
                'expected_vulnerabilities': 1,
                'expected_cwe': 'CWE-79'
            },
            {
                'name': 'safe_code',
                'description': 'Test that safe code is not flagged',
                'code': '''
def safe_query(user_id):
    import sqlite3
    conn = sqlite3.connect('db.sqlite')
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM users WHERE id = ?", (user_id,))
    return cursor.fetchall()
''',
                'expected_vulnerabilities': 0
            },
            {
                'name': 'inter_procedural',
                'description': 'Test inter-procedural vulnerability tracking',
                'code': '''
def get_input():
    return request.args.get('data')

def process_data():
    data = get_input()
    query = f"SELECT * FROM table WHERE col = '{data}'"
    cursor.execute(query)
''',
                'expected_vulnerabilities': 1,
                'expected_cwe': 'CWE-89'
            }
        ]

    def run_regression_tests(self) -> Dict[str, Any]:
        """Run the complete regression test suite."""
        print("ðŸ§ª RUNNING REGRESSION TEST SUITE")
        print("=" * 50)

        results = []
        total_start_time = time.time()

        for i, test_case in enumerate(self.test_cases, 1):
            print(f"Running test {i}/{len(self.test_cases)}: {test_case['name']}")
            result = self._run_single_test(test_case)
            results.append(result)

            status = "âœ… PASSED" if result['passed'] else "âŒ FAILED"
            print(f"   {status}: {result['message']}")

        total_time = time.time() - total_start_time

        # Analyze results
        analysis = self._analyze_regression_results(results)

        summary = {
            'total_tests': len(results),
            'passed_tests': len([r for r in results if r['passed']]),
            'failed_tests': len([r for r in results if not r['passed']]),
            'total_time': total_time,
            'results': results,
            'analysis': analysis
        }

        print("
ðŸ“Š REGRESSION TEST SUMMARY"        print(f"   Total Tests: {summary['total_tests']}")
        print(f"   Passed: {summary['passed_tests']}")
        print(f"   Failed: {summary['failed_tests']}")
        print(".2f")
        print(".1f")

        return summary

    def _run_single_test(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """Run a single regression test."""
        try:
            # Import and run ensemble analyzer
            from .ensemble_analyzer import EnsembleAnalyzer

            analyzer = EnsembleAnalyzer()
            files = [('test.py', test_case['code'])]

            start_time = time.time()
            vulnerabilities = analyzer.analyze_codebase(files)
            processing_time = time.time() - start_time

            # Validate results
            actual_vulns = len(vulnerabilities)
            expected_vulns = test_case['expected_vulnerabilities']

            passed = actual_vulns == expected_vulns

            if passed and expected_vulns > 0:
                # Check CWE if expected
                if 'expected_cwe' in test_case:
                    expected_cwe = test_case['expected_cwe']
                    found_cwe = any(v.cwe == expected_cwe for v in vulnerabilities)
                    if not found_cwe:
                        passed = False
                        message = f"Expected CWE {expected_cwe} not found"
                    else:
                        message = f"Found {actual_vulns} vulnerabilities as expected"
                else:
                    message = f"Found {actual_vulns} vulnerabilities as expected"
            elif passed and expected_vulns == 0:
                message = "No false positives detected"
            else:
                message = f"Expected {expected_vulns} vulnerabilities, found {actual_vulns}"

            return {
                'test_name': test_case['name'],
                'passed': passed,
                'message': message,
                'actual_vulnerabilities': actual_vulns,
                'expected_vulnerabilities': expected_vulns,
                'processing_time': processing_time,
                'vulnerabilities': [{'cwe': v.cwe, 'severity': v.severity} for v in vulnerabilities]
            }

        except Exception as e:
            return {
                'test_name': test_case['name'],
                'passed': False,
                'message': f"Test execution failed: {e}",
                'actual_vulnerabilities': 0,
                'expected_vulnerabilities': test_case['expected_vulnerabilities'],
                'processing_time': 0,
                'vulnerabilities': []
            }

    def _analyze_regression_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze regression test results."""
        passed_tests = [r for r in results if r['passed']]
        failed_tests = [r for r in results if not r['passed']]

        analysis = {
            'pass_rate': len(passed_tests) / len(results) if results else 0,
            'avg_processing_time': statistics.mean(r['processing_time'] for r in results) if results else 0,
            'failed_tests_details': failed_tests,
            'performance_trends': self._analyze_performance_trends(results),
            'recommendations': []
        }

        # Generate recommendations
        if analysis['pass_rate'] < 0.9:
            analysis['recommendations'].append("High failure rate - investigate test cases and scanner logic")

        if analysis['avg_processing_time'] > 10:
            analysis['recommendations'].append("Slow performance - consider optimization")

        return analysis

    def _analyze_performance_trends(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze performance trends across test runs."""
        # This would compare with historical data
        # For now, return basic analysis
        processing_times = [r['processing_time'] for r in results]

        return {
            'avg_time': statistics.mean(processing_times) if processing_times else 0,
            'std_time': statistics.stdev(processing_times) if len(processing_times) > 1 else 0,
            'trend': 'stable'  # Would analyze historical trends
        }

    def update_baselines(self, results: List[Dict[str, Any]]):
        """Update baseline expectations based on test results."""
        # This would update expected results based on confirmed behavior
        pass


class ConsistencyValidator:
    """Main consistency validation system."""

    def __init__(self):
        self.monitor = ContinuousMonitor()
        self.regression_suite = RegressionTestSuite()
        self.consistency_reports = []

    def run_full_validation(self) -> ConsistencyReport:
        """Run full consistency validation."""
        print("ðŸ” RUNNING FULL CONSISTENCY VALIDATION")
        print("=" * 60)

        start_time = datetime.now()

        # 1. Run regression tests
        print("ðŸ“‹ Phase 1: Regression Testing")
        regression_results = self.regression_suite.run_regression_tests()

        # 2. Performance monitoring
        print("\\nðŸ“Š Phase 2: Performance Monitoring")
        performance_report = self.monitor.get_performance_report()

        # 3. Consistency analysis
        print("\\nðŸ”„ Phase 3: Consistency Analysis")
        consistency_analysis = self._analyze_consistency(regression_results, performance_report)

        # 4. Generate recommendations
        print("\\nðŸ’¡ Phase 4: Recommendations")
        recommendations = self._generate_recommendations(consistency_analysis)

        # Create comprehensive report
        report = ConsistencyReport(
            timestamp=start_time,
            overall_consistency_score=consistency_analysis['overall_score'],
            performance_trends=consistency_analysis['trends'],
            regression_alerts=consistency_analysis['alerts'],
            anomaly_detections=consistency_analysis['anomalies'],
            recommendations=recommendations,
            next_test_schedule=start_time + timedelta(hours=24)  # Next daily test
        )

        self.consistency_reports.append(report)

        # Display results
        self._display_consistency_report(report)

        return report

    def _analyze_consistency(self, regression_results: Dict, performance_report: Dict) -> Dict[str, Any]:
        """Analyze overall consistency."""
        # Calculate consistency score
        regression_pass_rate = regression_results['passed_tests'] / regression_results['total_tests']

        # Performance consistency (simplified)
        perf_score = 0.9  # Would analyze performance stability

        overall_score = (regression_pass_rate + perf_score) / 2

        # Analyze trends
        trends = {}
        if 'performance_trends' in performance_report:
            trends = performance_report['performance_trends']

        # Check for alerts
        alerts = []
        if regression_pass_rate < 0.9:
            alerts.append(f"Low regression test pass rate: {regression_pass_rate:.1%}")

        # Check for anomalies
        anomalies = []
        if overall_score < 0.85:
            anomalies.append(f"Overall consistency below threshold: {overall_score:.1%}")

        return {
            'overall_score': overall_score,
            'regression_pass_rate': regression_pass_rate,
            'performance_score': perf_score,
            'trends': trends,
            'alerts': alerts,
            'anomalies': anomalies
        }

    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on analysis."""
        recommendations = []

        if analysis['overall_score'] < 0.9:
            recommendations.append("Overall consistency needs improvement - review failing tests")

        if analysis['regression_pass_rate'] < 0.95:
            recommendations.append("Improve test reliability and scanner accuracy")

        # Performance recommendations
        if 'scan_time_avg' in analysis.get('trends', {}):
            if analysis['trends']['scan_time_avg'] == 'degrading':
                recommendations.append("Scan time increasing - optimize performance")

        # Add general recommendations
        recommendations.extend([
            "Schedule daily automated testing",
            "Monitor performance trends continuously",
            "Update baselines when significant improvements are made",
            "Investigate and fix any regression alerts immediately"
        ])

        return recommendations

    def _display_consistency_report(self, report: ConsistencyReport):
        """Display the consistency report."""
        print("\\nðŸŽ¯ CONSISTENCY VALIDATION REPORT")
        print("=" * 50)
        print(f"Timestamp: {report.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        print(".1%")

        print("\\nðŸ“ˆ Performance Trends:")
        for metric, trend in report.performance_trends.items():
            print(f"   {metric}: {trend}")

        if report.regression_alerts:
            print("\\nâš ï¸ Regression Alerts:")
            for alert in report.regression_alerts:
                print(f"   â€¢ {alert}")

        if report.anomaly_detections:
            print("\\nðŸš¨ Anomalies Detected:")
            for anomaly in report.anomaly_detections:
                print(f"   â€¢ {anomaly}")

        print("\\nðŸ’¡ Recommendations:")
        for rec in report.recommendations:
            print(f"   â€¢ {rec}")

        print(f"\\nâ° Next Test Scheduled: {report.next_test_schedule.strftime('%Y-%m-%d %H:%M:%S')}")

    def start_continuous_monitoring(self):
        """Start continuous monitoring."""
        self.monitor.start_monitoring(interval_minutes=60)  # Hourly checks
        print("âœ… Continuous monitoring started")

    def stop_continuous_monitoring(self):
        """Stop continuous monitoring."""
        self.monitor.stop_monitoring()
        print("â¹ï¸ Continuous monitoring stopped")

    def add_alert_callback(self, callback: Callable):
        """Add alert callback."""
        self.monitor.add_alert_callback(callback)


def run_consistency_validation():
    """Main function to run consistency validation."""
    validator = ConsistencyValidator()

    # Run full validation
    report = validator.run_full_validation()

    return {
        'report': report,
        'validator': validator
    }


# Integration function
def validate_system_consistency():
    """Validate that Valid8 maintains consistent performance over time."""
    return run_consistency_validation()
