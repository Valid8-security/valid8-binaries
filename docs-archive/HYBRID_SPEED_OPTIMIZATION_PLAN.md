# Hybrid Mode Speed Optimization Plan

**Goal:** Achieve Snyk-level speed (83 files/sec) while maintaining 90.9% recall  
**Current:** 0.8 files/sec  
**Target:** 83-100 files/sec  
**Multiplier needed:** 100x speedup

---

## Current Architecture Issues

### Bottleneck #1: Sequential File Processing (CLI)
```python
# Line 149 in cli.py - MAJOR BOTTLENECK
for i, file_path in enumerate(scanned_files[:10]):  # Sequential, only 10 files!
    ai_detector.detect_vulnerabilities(code, filepath, language)
```

**Impact:** Processing files one-by-one instead of in parallel

### Bottleneck #2: Individual LLM Calls Per File
**Current:**
- Each file = separate LLM API call
- Ollama processes requests sequentially
- No batching

**Impact:** High latency dominates scan time

### Bottleneck #3: No Prioritization
**Current:** All files treated equally
**Impact:** AI time wasted on low-risk files

---

## Optimization Strategies (Ranked by Impact)

### Strategy 1: Parallel File Processing âœ… (10-50x speedup)

**Current:** Process files sequentially
```
File 1 â†’ wait â†’ File 2 â†’ wait â†’ File 3 ... (slow!)
```

**Optimized:** Process all files in parallel
```
File 1 â”
File 2 â”œâ†’ All processed simultaneously
File 3 â”˜ (fast!)
```

**Implementation:**
```python
# parry/cli.py - Replace sequential with parallel
from concurrent.futures import ThreadPoolExecutor

def process_file_parallel(file_path, ai_detector):
    code = file_path.read_text(errors='ignore')
    return ai_detector.detect_vulnerabilities(code, str(file_path), file_path.suffix[1:])

# Process all files in parallel
with ThreadPoolExecutor(max_workers=16) as executor:
    futures = [executor.submit(process_file_parallel, f, ai_detector) 
               for f in scanned_files]
    ai_vulns = []
    for future in as_completed(futures):
        ai_vulns.extend(future.result())
```

**Speed Impact:** 10-16x faster on multi-core systems

---

### Strategy 2: Batch LLM Requests (5-10x speedup)

**Current:** One API call per file
```
File 1 â†’ LLM API â†’ wait â†’ response
File 2 â†’ LLM API â†’ wait â†’ response
```

**Optimized:** Batch multiple files in one request
```
Files 1-10 â†’ Single LLM API â†’ All responses
```

**Implementation:**
```python
# parry/llm.py - Add batch generation method
def generate_batch(self, prompts: List[str]) -> List[str]:
    """Process multiple prompts in one request"""
    payload = {
        "model": self.config.model,
        "prompt": "Analyze multiple files for vulnerabilities:\n\n" + "\n---\n".join(prompts),
        "num_predict": 8192,  # Larger for batch
        "stream": False
    }
    # ... implementation
```

**Speed Impact:** 5-10x reduction in API overhead

---

### Strategy 3: Smart File Prioritization (2-5x speedup)

**Current:** AI analyzes ALL files equally

**Optimized:** AI focuses on high-risk files only

**Risk Scoring:**
- High risk: Auth handlers, payment processors, user input handlers
- Medium risk: API endpoints, database queries
- Low risk: Utility functions, helpers, tests

**Implementation:**
```python
def prioritize_files(files, risk_keywords):
    """Score files by risk level"""
    scored = []
    for f in files:
        code = f.read_text(errors='ignore')
        score = 0
        
        # High-value keywords
        if any(kw in code for kw in ['password', 'secret', 'api_key', 'authenticate']):
            score += 100
        if any(kw in code for kw in ['payment', 'transaction', 'billing']):
            score += 80
        if any(kw in code for kw in ['db.execute', 'SQL', 'query']):
            score += 50
            
        scored.append((score, f))
    
    # Sort by score, AI analyze top 30% only
    scored.sort(reverse=True)
    high_risk = [f for s, f in scored[:len(scored)//3]]
    medium_risk = [f for s, f in scored[len(scored)//3:len(scored)*2//3]]
    low_risk = [f for s, f in scored[len(scored)*2//3:]]
    
    return high_risk, medium_risk, low_risk
```

**AI Analysis Priority:**
1. High risk: Full AI analysis
2. Medium risk: Lightweight AI analysis
3. Low risk: Pattern-only (Fast Mode)

**Speed Impact:** AI time reduced by 60-70%, recall stays ~90%

---

### Strategy 4: Incremental Context Window (3-5x speedup)

**Current:** Each file analyzed in isolation

**Optimized:** Context-aware scanning reduces redundant analysis

**Implementation:**
```python
# Cache common patterns across files
class ContextualAIDetector:
    def __init__(self):
        self.file_tree = {}  # Project structure
        self.dependency_map = {}  # Import relationships
        self.common_patterns = {}  # Repeated code patterns
        
    def analyze_with_context(self, file_path, code, language):
        # Pre-fetch related files
        related_files = self._get_related_files(file_path)
        context = self._build_context(related_files)
        
        # Analyze with context (single LLM call for related files)
        return self._analyze_contextual(code, context)
```

**Speed Impact:** 3-5x fewer LLM calls needed

---

### Strategy 5: Model Optimization (2-3x speedup)

**Current:** CodeLlama 7B, full inference

**Optimized Options:**

**Option A: Quantized Models**
```bash
# Download 4-bit quantized model (50% faster, same quality)
ollama pull codellama:7b-instruct-q4_K_M
```

**Option B: Smaller Context**
```python
# Reduce max_tokens for faster inference
llm_config.max_tokens = 512  # Instead of 2048
```

**Option C: Temperature = 0**
```python
# Deterministic = faster + same results
llm_config.temperature = 0.0  # Instead of 0.1
```

**Speed Impact:** 2-3x faster inference

---

## Combined Optimization Results

### Scenarios

| Scenario | Current | Optimized | Improvement |
|----------|---------|-----------|-------------|
| **50 files** | 1 minute | **5-8 seconds** | 10-12x |
| **500 files** | 10 minutes | **30-60 seconds** | 10-20x |
| **5,000 files** | 2 hours | **5-15 minutes** | 8-24x |

### Expected Performance After All Optimizations

| Metric | Before | After | Status |
|--------|--------|-------|--------|
| **Speed** | 0.8 files/s | **50-100 files/s** | âœ… Near Snyk |
| **Recall** | 90.9% | **88-92%** | âœ… Maintained |
| **Parallel Files** | 1 | 16-32 | âœ… |
| **Batch Processing** | No | Yes | âœ… |
| **Smart Prioritization** | No | Yes | âœ… |

---

## Implementation Priority

### Phase 1: Quick Wins (1 day)
1. âœ… Remove 10-file limit
2. âœ… Implement parallel file processing
3. âœ… Add quantized model support

**Expected:** 10-15x speedup

---

### Phase 2: Core Optimizations (3-5 days)
4. âœ… Batch LLM requests
5. âœ… Smart file prioritization
6. âœ… Incremental context window

**Expected:** Additional 5-10x speedup

**Total:** 50-150x speedup

---

### Phase 3: Advanced Optimizations (1-2 weeks)
7. âœ… GPU acceleration
8. âœ… Distributed scanning
9. âœ… Persistent detection cache
10. âœ… Predictive batching

**Expected:** Additional 2-5x speedup

**Total:** 100-750x speedup potential

---

## Preserving Recall: Smart AI Allocation

### Strategy: Hybrid-Fast Approach

**Theory:** Not all files need full AI analysis

**Implementation:**
```
For each file:
  1. Run Fast Mode (pattern-based, 0.1s)
  2. Calculate risk score
  3. If high-risk â†’ Full AI analysis
  4. If medium-risk â†’ Lightweight AI (shorter context)
  5. If low-risk â†’ Pattern-only
```

**Risk Scoring Example:**
```python
def calculate_risk_score(code, language):
    """Score file by vulnerability likelihood"""
    score = 0
    
    # High-risk patterns
    if re.search(r'(eval|exec|shell_exec)', code):
        score += 50
    
    if re.search(r'(password|secret|api_key)', code):
        score += 40
        
    # User input handling
    if re.search(r'(request\.|POST|GET|Input)', code):
        score += 30
        
    # Database operations
    if re.search(r'(query|execute|sql)', code, re.IGNORECASE):
        score += 20
        
    return score

# AI analysis thresholds
if risk_score > 80:
    # Full AI analysis (all CWEs)
    return full_ai_scan(file)
elif risk_score > 50:
    # Targeted AI (top 20 CWEs)
    return targeted_ai_scan(file, top_cwes=20)
else:
    # Fast mode only
    return pattern_scan(file)
```

**Result:** AI time reduced by 70%, recall maintained at 88-92%

---

## Architecture: Parallel Scanning

### New Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HYBRID MODE OPTIMIZED                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 1: Fast Pattern Scan (all files, parallel)
   â†“
   â”œâ”€â†’ 222 files/sec
   â”œâ”€â†’ All 47 CWEs
   â””â”€â†’ Recall: 72.7%

Phase 2: Risk Assessment (parallel)
   â†“
   â”œâ”€â†’ Score all files
   â”œâ”€â†’ Classify: High/Medium/Low
   â””â”€â†’ Sort by risk

Phase 3: Smart AI Analysis (parallel batches)
   â†“
   â”œâ”€â†’ High-risk: Full AI (16 workers)
   â”œâ”€â†’ Medium-risk: Lightweight AI (8 workers)
   â””â”€â†’ Low-risk: Skip AI

Phase 4: Merge & Deduplicate (fast)
   â†“
   â”œâ”€â†’ Combine results
   â”œâ”€â†’ Remove duplicates
   â””â”€â†’ Recall: 88-92%

Total Time: ~1 second (pattern) + AI time (optimized)
```

---

## Expected Performance After Optimizations

### Small Codebase (50 files)

| Implementation | Time | Speed | Recall |
|----------------|------|-------|--------|
| Current | 1 min | 0.8/s | 90.9% |
| Phase 1 | 5-8 sec | 6-10/s | 90.9% |
| Phase 2 | 2-5 sec | 10-25/s | 88-92% |
| Phase 3 | 1-3 sec | 17-50/s | 88-92% |

### Medium Codebase (500 files)

| Implementation | Time | Speed | Recall |
|----------------|------|-------|--------|
| Current | 10 min | 0.8/s | 90.9% |
| Phase 1 | 30-60 sec | 8-17/s | 90.9% |
| Phase 2 | 15-30 sec | 17-33/s | 88-92% |
| Phase 3 | 5-15 sec | 33-100/s | 88-92% |

**Target: Snyk speed (83 files/sec) - ACHIEVABLE**

---

## Trade-offs Analysis

### Recall Impact

| Optimization | Speed Gain | Recall Impact | Trade-off |
|--------------|------------|---------------|-----------|
| Parallel processing | 10-16x | 0% | âœ… None |
| Batch requests | 5-10x | 0% | âœ… None |
| Smart prioritization | 3-5x | 2-5% | âš ï¸ Small |
| Model quantization | 2-3x | 0% | âœ… None |
| Context window | 3-5x | 1-3% | âš ï¸ Small |
| **Combined** | **50-250x** | **2-8%** | âš ï¸ **Acceptable** |

**Bottom Line:** 88-92% recall vs Snyk's 50% recall = Still 76% better!

---

## Implementation Code Example

### Parallel File Processing (Phase 1)

```python
# parry/cli.py
from concurrent.futures import ThreadPoolExecutor, as_completed

def run_hybrid_scan_optimized(path, ai_detector, max_files=None):
    """Optimized hybrid scan with parallel processing"""
    
    # Phase 1: Fast pattern scan (already parallel in Scanner)
    pattern_results = scanner.scan(path)
    scanned_files = get_scanned_files(path)
    
    # Limit files if specified
    if max_files:
        scanned_files = scanned_files[:max_files]
    
    # Phase 2: Parallel AI analysis
    ai_vulns = []
    
    def process_file(file_path):
        """Process single file with AI"""
        try:
            code = file_path.read_text(errors='ignore')
            vulns = ai_detector.detect_vulnerabilities(
                code, str(file_path), file_path.suffix[1:]
            )
            return [v.to_dict() if hasattr(v, 'to_dict') else v for v in vulns]
        except Exception as e:
            console.print(f"[yellow]Warning: {e}[/yellow]")
            return []
    
    # Process all files in parallel (16 workers)
    with ThreadPoolExecutor(max_workers=16) as executor:
        futures = {executor.submit(process_file, f): f for f in scanned_files}
        
        for future in as_completed(futures):
            file_vulns = future.result()
            ai_vulns.extend(file_vulns)
    
    # Merge and deduplicate
    combined = pattern_results['vulnerabilities'] + ai_vulns
    deduped = deduplicate_vulnerabilities(combined)
    
    return deduped
```

**Expected Speed:** 10-15x faster with zero recall loss

---

## Testing & Validation

### Benchmark Test Suite

```python
# tests/test_performance.py

def test_parallel_speedup():
    """Verify parallel processing improves speed"""
    
    codebase = create_test_codebase(100_files)
    
    # Sequential (current)
    start = time.time()
    sequential_scan(codebase)
    sequential_time = time.time() - start
    
    # Parallel (optimized)
    start = time.time()
    parallel_scan(codebase, workers=16)
    parallel_time = time.time() - start
    
    # Assert speedup
    assert parallel_time < sequential_time / 8  # At least 8x faster
    assert recall_parallel >= recall_sequential * 0.95  # 95% recall maintained
```

---

## Conclusion

### Feasibility: âœ… HIGH

**Can we reach Snyk speed (83 files/sec) while keeping 90% recall?**

**Answer:** **YES** with smart optimizations.

### Path Forward

1. **Phase 1 (1 day):** Remove limit, add parallel processing â†’ **15-30 files/sec**
2. **Phase 2 (1 week):** Smart prioritization, batching â†’ **50-100 files/sec** âœ…
3. **Phase 3 (optional):** GPU, advanced caching â†’ **100+ files/sec**

### Expected Final Performance

| Metric | Target | Snyk | Status |
|--------|--------|------|--------|
| **Speed** | 83+ files/sec | 83 files/sec | âœ… **Meet/Beat** |
| **Recall** | 88-92% | 50% | âœ… **75% Better** |
| **Privacy** | 100% local | 0% | âœ… **Advantage** |
| **Cost** | $99/mo | $200+/mo | âœ… **Advantage** |

**Verdict:** With optimizations, Parry Hybrid can match Snyk speed while being **76% more accurate**! ğŸš€

---

**Next Steps:** Implement Phase 1 optimizations (parallel file processing) for immediate 10-15x speedup.

