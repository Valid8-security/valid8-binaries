{
  "valid8_results": [
    {
      "benchmark_name": "GroundTruth-javascript-large",
      "dataset": "Real Ground Truth",
      "language": "javascript",
      "total_files": 100,
      "total_vulnerabilities": 300,
      "detected_vulnerabilities": 0,
      "true_positives": 0,
      "false_positives": 0,
      "false_negatives": 300,
      "precision": 0,
      "recall": 0.0,
      "f1_score": 0,
      "scan_time_seconds": 111.42,
      "files_per_second": 0.9,
      "timestamp": "2025-11-26T09:36:05.585766",
      "source": "Ground Truth Benchmark"
    },
    {
      "benchmark_name": "GroundTruth-javascript-medium",
      "dataset": "Real Ground Truth",
      "language": "javascript",
      "total_files": 25,
      "total_vulnerabilities": 75,
      "detected_vulnerabilities": 0,
      "true_positives": 0,
      "false_positives": 0,
      "false_negatives": 75,
      "precision": 0,
      "recall": 0.0,
      "f1_score": 0,
      "scan_time_seconds": 28.33,
      "files_per_second": 0.88,
      "timestamp": "2025-11-26T09:36:33.925931",
      "source": "Ground Truth Benchmark"
    },
    {
      "benchmark_name": "GroundTruth-kotlin-large",
      "dataset": "Real Ground Truth",
      "language": "kotlin",
      "total_files": 100,
      "total_vulnerabilities": 300,
      "detected_vulnerabilities": 0,
      "true_positives": 0,
      "false_positives": 0,
      "false_negatives": 300,
      "precision": 0,
      "recall": 0.0,
      "f1_score": 0,
      "scan_time_seconds": 90.46,
      "files_per_second": 1.11,
      "timestamp": "2025-11-26T09:38:04.390456",
      "source": "Ground Truth Benchmark"
    },
    {
      "benchmark_name": "GroundTruth-kotlin-medium",
      "dataset": "Real Ground Truth",
      "language": "kotlin",
      "total_files": 25,
      "total_vulnerabilities": 75,
      "detected_vulnerabilities": 0,
      "true_positives": 0,
      "false_positives": 0,
      "false_negatives": 75,
      "precision": 0,
      "recall": 0.0,
      "f1_score": 0,
      "scan_time_seconds": 32.15,
      "files_per_second": 0.78,
      "timestamp": "2025-11-26T09:38:36.552640",
      "source": "Ground Truth Benchmark"
    },
    {
      "benchmark_name": "GroundTruth-kotlin-small",
      "dataset": "Real Ground Truth",
      "language": "kotlin",
      "total_files": 5,
      "total_vulnerabilities": 15,
      "detected_vulnerabilities": 0,
      "true_positives": 0,
      "false_positives": 0,
      "false_negatives": 15,
      "precision": 0,
      "recall": 0.0,
      "f1_score": 0,
      "scan_time_seconds": 4.92,
      "files_per_second": 1.02,
      "timestamp": "2025-11-26T09:38:41.480900",
      "source": "Ground Truth Benchmark"
    },
    {
      "benchmark_name": "GroundTruth-java-large",
      "dataset": "Real Ground Truth",
      "language": "java",
      "total_files": 100,
      "total_vulnerabilities": 300,
      "detected_vulnerabilities": 0,
      "true_positives": 0,
      "false_positives": 0,
      "false_negatives": 300,
      "precision": 0,
      "recall": 0.0,
      "f1_score": 0,
      "scan_time_seconds": 111.26,
      "files_per_second": 0.9,
      "timestamp": "2025-11-26T09:40:32.747149",
      "source": "Ground Truth Benchmark"
    },
    {
      "benchmark_name": "GroundTruth-java-medium",
      "dataset": "Real Ground Truth",
      "language": "java",
      "total_files": 25,
      "total_vulnerabilities": 75,
      "detected_vulnerabilities": 0,
      "true_positives": 0,
      "false_positives": 0,
      "false_negatives": 75,
      "precision": 0,
      "recall": 0.0,
      "f1_score": 0,
      "scan_time_seconds": 23.41,
      "files_per_second": 1.07,
      "timestamp": "2025-11-26T09:40:56.165508",
      "source": "Ground Truth Benchmark"
    },
    {
      "benchmark_name": "GroundTruth-typescript-large",
      "dataset": "Real Ground Truth",
      "language": "typescript",
      "total_files": 100,
      "total_vulnerabilities": 300,
      "detected_vulnerabilities": 0,
      "true_positives": 0,
      "false_positives": 0,
      "false_negatives": 300,
      "precision": 0,
      "recall": 0.0,
      "f1_score": 0,
      "scan_time_seconds": 101.07,
      "files_per_second": 0.99,
      "timestamp": "2025-11-26T09:42:37.238530",
      "source": "Ground Truth Benchmark"
    },
    {
      "benchmark_name": "GroundTruth-typescript-medium",
      "dataset": "Real Ground Truth",
      "language": "typescript",
      "total_files": 25,
      "total_vulnerabilities": 75,
      "detected_vulnerabilities": 0,
      "true_positives": 0,
      "false_positives": 0,
      "false_negatives": 75,
      "precision": 0,
      "recall": 0.0,
      "f1_score": 0,
      "scan_time_seconds": 21.07,
      "files_per_second": 1.19,
      "timestamp": "2025-11-26T09:42:58.310990",
      "source": "Ground Truth Benchmark"
    },
    {
      "benchmark_name": "GroundTruth-typescript-small",
      "dataset": "Real Ground Truth",
      "language": "typescript",
      "total_files": 5,
      "total_vulnerabilities": 15,
      "detected_vulnerabilities": 0,
      "true_positives": 0,
      "false_positives": 0,
      "false_negatives": 15,
      "precision": 0,
      "recall": 0.0,
      "f1_score": 0,
      "scan_time_seconds": 4.44,
      "files_per_second": 1.13,
      "timestamp": "2025-11-26T09:43:02.755867",
      "source": "Ground Truth Benchmark"
    },
    {
      "benchmark_name": "GroundTruth-python-large",
      "dataset": "Real Ground Truth",
      "language": "python",
      "total_files": 100,
      "total_vulnerabilities": 300,
      "detected_vulnerabilities": 0,
      "true_positives": 0,
      "false_positives": 0,
      "false_negatives": 300,
      "precision": 0,
      "recall": 0.0,
      "f1_score": 0,
      "scan_time_seconds": 99.77,
      "files_per_second": 1.0,
      "timestamp": "2025-11-26T09:44:42.537313",
      "source": "Ground Truth Benchmark"
    },
    {
      "benchmark_name": "GroundTruth-python-medium",
      "dataset": "Real Ground Truth",
      "language": "python",
      "total_files": 25,
      "total_vulnerabilities": 75,
      "detected_vulnerabilities": 0,
      "true_positives": 0,
      "false_positives": 0,
      "false_negatives": 75,
      "precision": 0,
      "recall": 0.0,
      "f1_score": 0,
      "scan_time_seconds": 25.02,
      "files_per_second": 1.0,
      "timestamp": "2025-11-26T09:45:07.570376",
      "source": "Ground Truth Benchmark"
    }
  ],
  "competitor_data": [
    {
      "tool_name": "Semgrep",
      "benchmark": "OWASP Benchmark",
      "precision": 0.85,
      "recall": 0.78,
      "f1_score": 0.81,
      "speed_files_per_second": 1500,
      "source": "Semgrep Blog 2023",
      "year": 2023
    },
    {
      "tool_name": "CodeQL",
      "benchmark": "OWASP Benchmark",
      "precision": 0.92,
      "recall": 0.71,
      "f1_score": 0.8,
      "speed_files_per_second": 450,
      "source": "GitHub Research 2023",
      "year": 2023
    },
    {
      "tool_name": "SonarQube",
      "benchmark": "OWASP Benchmark",
      "precision": 0.78,
      "recall": 0.85,
      "f1_score": 0.81,
      "speed_files_per_second": 890,
      "source": "SonarQube Docs 2023",
      "year": 2023
    },
    {
      "tool_name": "Checkmarx",
      "benchmark": "OWASP Benchmark",
      "precision": 0.88,
      "recall": 0.76,
      "f1_score": 0.81,
      "speed_files_per_second": 320,
      "source": "Checkmarx Report 2023",
      "year": 2023
    },
    {
      "tool_name": "Semgrep",
      "benchmark": "Juliet Test Suite",
      "precision": 0.82,
      "recall": 0.91,
      "f1_score": 0.86,
      "speed_files_per_second": 2100,
      "source": "NIST Report 2023",
      "year": 2023
    },
    {
      "tool_name": "CodeQL",
      "benchmark": "Juliet Test Suite",
      "precision": 0.95,
      "recall": 0.68,
      "f1_score": 0.79,
      "speed_files_per_second": 380,
      "source": "GitHub Research 2023",
      "year": 2023
    },
    {
      "tool_name": "SonarQube",
      "benchmark": "Juliet Test Suite",
      "precision": 0.76,
      "recall": 0.88,
      "f1_score": 0.81,
      "speed_files_per_second": 1200,
      "source": "SonarQube Docs 2023",
      "year": 2023
    },
    {
      "tool_name": "Semgrep",
      "benchmark": "Real World",
      "precision": 0.79,
      "recall": 0.84,
      "f1_score": 0.81,
      "speed_files_per_second": 1800,
      "source": "Industry Benchmarks 2023",
      "year": 2023
    },
    {
      "tool_name": "CodeQL",
      "benchmark": "Real World",
      "precision": 0.89,
      "recall": 0.73,
      "f1_score": 0.8,
      "speed_files_per_second": 420,
      "source": "GitHub Research 2023",
      "year": 2023
    },
    {
      "tool_name": "SonarQube",
      "benchmark": "Real World",
      "precision": 0.81,
      "recall": 0.79,
      "f1_score": 0.8,
      "speed_files_per_second": 950,
      "source": "SonarQube Enterprise",
      "year": 2023
    },
    {
      "tool_name": "Checkmarx",
      "benchmark": "Real World",
      "precision": 0.86,
      "recall": 0.77,
      "f1_score": 0.81,
      "speed_files_per_second": 310,
      "source": "Checkmarx CxSAST",
      "year": 2023
    },
    {
      "tool_name": "Fortify",
      "benchmark": "Real World",
      "precision": 0.9,
      "recall": 0.75,
      "f1_score": 0.82,
      "speed_files_per_second": 280,
      "source": "Micro Focus Report",
      "year": 2023
    }
  ],
  "generated_at": "2025-11-25T22:26:03.910232",
  "summary": {
    "valid8_avg_precision": 0,
    "valid8_avg_recall": 0.0,
    "valid8_avg_f1": 0,
    "valid8_avg_speed": 1.0,
    "total_benchmarks": 3,
    "total_competitors": 12,
    "ground_truth_validated": true,
    "methodology": "Ground truth comparison with hybrid scanning"
  }
}